{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "no_actions = env.action_space.n\n",
    "no_observations = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# noinspection PyShadowingNames\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.replay_memory = deque(maxlen=EXPERIENCE_SIZE)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(no_observations, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, no_actions)\n",
    "        )\n",
    "        self.net = self.net.cuda()\n",
    "        self.optim = optim.Adam(self.net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    def act(self, state, test=False):\n",
    "        if (np.random.uniform(0, 1) > EPSILON) or test:\n",
    "            q_values = self.net(torch.tensor(state).float().unsqueeze(0).cuda())\n",
    "            action = torch.argmax(q_values.squeeze()).item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def learn(self, episode):\n",
    "        if len(self.replay_memory) < BATCH_SIZE:\n",
    "            return\n",
    "        train_batch = random.sample(self.replay_memory, BATCH_SIZE)\n",
    "\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*train_batch)\n",
    "\n",
    "        batch_state = torch.tensor(batch_state).float().cuda()\n",
    "        batch_next_state = torch.tensor(batch_next_state).float().cuda()\n",
    "        batch_action = torch.tensor(batch_action).cuda().unsqueeze(1)\n",
    "        batch_done = torch.tensor(batch_done).cuda().to(dtype=torch.int)\n",
    "        batch_reward = torch.tensor(batch_reward).cuda().to(dtype=torch.int)\n",
    "\n",
    "        current_q_values = self.net(batch_state).gather(1, batch_action)\n",
    "        target_q_values = batch_reward + (GAMMA * self.net(batch_next_state).max(1)[0])\n",
    "#         target_q_values = batch_reward + (torch.ones_like(batch_done) - batch_done) * (GAMMA * self.net(batch_next_state).max(1)[0])\n",
    "\n",
    "        loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
    "        if not episode%30:\n",
    "            print(loss)\n",
    "\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "dqn = DQN()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%"
    }
   },
   "outputs": [],
   "source": [
    "# noinspection PyUnusedLocal,PyShadowingNames\n",
    "def test():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = dqn.act(state, True)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "#         time.sleep(0.07)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    env.close()\n",
    "    print(f\"Evaluation Score : {total_reward}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0\n",
      "Best Reward : 33.0\n",
      "Mean over last 50 : 33.0\n",
      "Epsilon : 1.0\n",
      "\n",
      "Evaluation Score : 9.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syzygianinfern0/miniconda3/envs/rl/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4825, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Episode : 30\n",
      "Best Reward : 33.0\n",
      "Mean over last 50 : 19.548387096774192\n",
      "Epsilon : 0.8621008966608072\n",
      "\n",
      "Evaluation Score : 9.0\n",
      "\n",
      "tensor(0.4953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Episode : 60\n",
      "Best Reward : 50.0\n",
      "Mean over last 50 : 20.81967213114754\n",
      "Epsilon : 0.7434100384749007\n",
      "\n",
      "Evaluation Score : 74.0\n",
      "\n",
      "tensor(0.4936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4469, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4457, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5329, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4375, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4577, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Episode : 90\n",
      "Best Reward : 76.0\n",
      "Mean over last 50 : 21.747252747252748\n",
      "Epsilon : 0.6412518701055556\n",
      "\n",
      "Evaluation Score : 9.0\n",
      "\n",
      "tensor(0.4836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4788, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5130, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Episode : 120\n",
      "Best Reward : 76.0\n",
      "Mean over last 50 : 23.67\n",
      "Epsilon : 0.5533235197330861\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Score : 10.0\n",
      "\n",
      "tensor(0.5174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4825, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Episode : 150\n",
      "Best Reward : 88.0\n",
      "Mean over last 50 : 23.64\n",
      "Epsilon : 0.47764288721360454\n",
      "\n",
      "Evaluation Score : 11.0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0131ca167c46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mEPSILON\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMAX_EPSILON\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mDECAY_RATE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2bcd3d6b5144>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#         target_q_values = batch_reward + (torch.ones_like(batch_done) - batch_done) * (GAMMA * self.net(batch_next_state).max(1)[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msmooth_l1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2149\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_smooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_smooth_l1_loss\u001b[0;34m(input, target)\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \u001b[0;31m# type: (Tensor, Tensor) -> Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2130\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2131\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "rewards_dq = deque(maxlen=100)\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    # noinspection PyRedeclaration\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = dqn.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        dqn.remember(state, action, reward, next_state, done)\n",
    "        dqn.learn(episode)\n",
    "        EPSILON = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-1 * DECAY_RATE * episode)\n",
    "        state = next_state\n",
    "        \n",
    "    rewards.append(total_reward)\n",
    "    rewards_dq.append(total_reward)\n",
    "    \n",
    "    if not episode % 30:\n",
    "        print(f'Episode : {episode}')\n",
    "        print(f'Best Reward : {max(rewards)}')\n",
    "        print(f'Mean over last 50 : {np.mean(rewards_dq)}')\n",
    "        print(f'Epsilon : {EPSILON}')\n",
    "        print()\n",
    "        if np.mean(rewards_dq) > 195:\n",
    "            break\n",
    "        test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "rewards_dq = deque(maxlen=100)\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    # noinspection PyRedeclaration\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = dqn.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        dqn.remember(state, action, reward, next_state, done)\n",
    "        dqn.learn(episode)\n",
    "        EPSILON = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-1 * DECAY_RATE * episode)\n",
    "        state = next_state\n",
    "        \n",
    "    rewards.append(total_reward)\n",
    "    rewards_dq.append(total_reward)\n",
    "    \n",
    "    if not episode % 30:\n",
    "        print(f'Episode : {episode}')\n",
    "        print(f'Best Reward : {max(rewards)}')\n",
    "        print(f'Mean over last 50 : {np.mean(rewards_dq)}')\n",
    "        print(f'Epsilon : {EPSILON}')\n",
    "        print()\n",
    "        if np.mean(rewards_dq) > 195:\n",
    "            break\n",
    "        test()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}