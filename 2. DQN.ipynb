{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "no_actions = env.action_space.n\n",
    "no_observations = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_EPISODES = 1000\n",
    "EXPERIENCE_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 1.0\n",
    "MAX_EPSILON = EPSILON = 1.0\n",
    "MIN_EPSILON = 1e-2\n",
    "DECAY_RATE = 0.005\n",
    "LEARNING_RATE = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# noinspection PyShadowingNames\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.replay_memory = deque(maxlen=EXPERIENCE_SIZE)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(no_observations, 256),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, no_actions)\n",
    "        )\n",
    "        self.net = self.net.cuda()\n",
    "        self.optim = optim.Adam(self.net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    def act(self, state, test=False):\n",
    "        if (np.random.uniform(0, 1) > EPSILON) or test:\n",
    "            q_values = self.net(torch.tensor(state).float().unsqueeze(0).cuda())\n",
    "            action = torch.argmax(q_values.squeeze()).item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def learn(self, episode):\n",
    "        if len(self.replay_memory) < BATCH_SIZE:\n",
    "            return\n",
    "        train_batch = random.sample(self.replay_memory, BATCH_SIZE)\n",
    "\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*train_batch)\n",
    "\n",
    "        batch_state = torch.tensor(batch_state).float().cuda()\n",
    "        batch_next_state = torch.tensor(batch_next_state).float().cuda()\n",
    "        batch_action = torch.tensor(batch_action).cuda().unsqueeze(1)\n",
    "        batch_done = torch.tensor(batch_done).cuda().to(dtype=torch.int)\n",
    "        batch_reward = torch.tensor(batch_reward).cuda().to(dtype=torch.int)\n",
    "\n",
    "        current_q_values = self.net(batch_state).gather(1, batch_action)\n",
    "        target_q_values = batch_reward + (GAMMA * self.net(batch_next_state).max(1)[0])\n",
    "#         target_q_values = batch_reward + (torch.ones_like(batch_done) - batch_done) * (GAMMA * self.net(batch_next_state).max(1)[0])\n",
    "\n",
    "        loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
    "        if not episode%30:\n",
    "            print(loss)\n",
    "\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "dqn = DQN()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%"
    }
   },
   "outputs": [],
   "source": [
    "# noinspection PyUnusedLocal,PyShadowingNames\n",
    "def test():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = dqn.act(state, True)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.07)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    env.close()\n",
    "    print(f\"Evaluation Score : {total_reward}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0\n",
      "Best Reward : 29.0\n",
      "Mean over last 50 : 29.0\n",
      "Epsilon : 1.0\n",
      "\n",
      "Evaluation Score : 10.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syzygianinfern0/miniconda3/envs/rl/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Episode : 30\n",
      "Best Reward : 48.0\n",
      "Mean over last 50 : 22.64516129032258\n",
      "Epsilon : 0.8621008966608072\n",
      "\n",
      "Evaluation Score : 20.0\n",
      "\n",
      "tensor(0.4954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Episode : 60\n",
      "Best Reward : 58.0\n",
      "Mean over last 50 : 23.049180327868854\n",
      "Epsilon : 0.7434100384749007\n",
      "\n",
      "Evaluation Score : 11.0\n",
      "\n",
      "tensor(0.4731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Episode : 90\n",
      "Best Reward : 58.0\n",
      "Mean over last 50 : 22.692307692307693\n",
      "Epsilon : 0.6412518701055556\n",
      "\n",
      "Evaluation Score : 18.0\n",
      "\n",
      "tensor(0.4959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Episode : 120\n",
      "Best Reward : 75.0\n",
      "Mean over last 50 : 22.76\n",
      "Epsilon : 0.5533235197330861\n",
      "\n",
      "Evaluation Score : 11.0\n",
      "\n",
      "tensor(0.4843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4819, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Episode : 150\n",
      "Best Reward : 75.0\n",
      "Mean over last 50 : 22.81\n",
      "Epsilon : 0.47764288721360454\n",
      "\n",
      "Evaluation Score : 9.0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0131ca167c46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mEPSILON\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMAX_EPSILON\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mDECAY_RATE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c5f40ce534aa>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#         target_q_values = batch_reward + (torch.ones_like(batch_done) - batch_done) * (GAMMA * self.net(batch_next_state).max(1)[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msmooth_l1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2149\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_smooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_smooth_l1_loss\u001b[0;34m(input, target)\u001b[0m\n\u001b[1;32m   2128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_smooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \u001b[0;31m# type: (Tensor, Tensor) -> Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2130\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "rewards_dq = deque(maxlen=100)\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    # noinspection PyRedeclaration\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = dqn.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        dqn.remember(state, action, reward, next_state, done)\n",
    "        dqn.learn(episode)\n",
    "        EPSILON = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-1 * DECAY_RATE * episode)\n",
    "        state = next_state\n",
    "        \n",
    "    rewards.append(total_reward)\n",
    "    rewards_dq.append(total_reward)\n",
    "    \n",
    "    if not episode % 30:\n",
    "        print(f'Episode : {episode}')\n",
    "        print(f'Best Reward : {max(rewards)}')\n",
    "        print(f'Mean over last 50 : {np.mean(rewards_dq)}')\n",
    "        print(f'Epsilon : {EPSILON}')\n",
    "        print()\n",
    "        if np.mean(rewards_dq) > 195:\n",
    "            break\n",
    "        test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
